
% =======================================================================================
\chapter{Funzioni $f:\mathbb{R} \rightarrow \mathbb{R}$}

In questo capitolo vengono elencate alcune funzioni elementari.

\section{Definizioni}
\begin{definition}[Funzioni pari e dispari]
\end{definition}


\begin{definition}[Funzione invertibile e funzione inversa]
\end{definition}

\section{Funzioni elementari}
\subsection{Potenza}
$f(x) = x^a$
\subsection{Radice}
{\color{red} Inversa della potenza. Dominio? Nel caso in cui la potenza non sia una funzione invertibile, quale ramo?}
$f(x) = \sqrt[a]{x}$
\subsection{Esponenziale}
$f(x) = a^x$
$f(x) = e^x$
\subsection{Logaritmo}
{\color{red} Inversa dell'esponenziale. Dominio \dots}


\subsection{Funzioni armoniche}
{\color{red} Dalla geometria \dots}

\noindent
Vengono definite le funzioni \textit{seno} e \textit{coseno},
\begin{equation}
\begin{aligned}
    \sin x &= \frac{\overline{PH}}{R} \\
    \cos x &= \frac{\overline{OH}}{R} \\
\end{aligned}
\end{equation}
Viene defita la funzione \textit{tangente}, per valori della variabile indipendente $x \neq \dfrac{\pi}{2} + n \pi$, $n \in \mathbb{Z}$,
\begin{equation}
    \tan x = \frac{\overline{PH}}{\overline{OH}} = \frac{\sin x}{\cos x}
\end{equation}

\subsubsection{Proprietà}
\paragraph{Identità fondamentale.} Usando il teorema di Pitagora, è immediato dimostrare l'identità fondamentale delle funzioni armoniche,
\begin{equation}
    \sin^2 x + \cos^2 x = 1 \ .
\end{equation}

\paragraph{Funzioni armoniche di somme e differenze.}
{\color{red} Dimostrazione geometrica}
\begin{equation}
\begin{aligned}
    \cos(x \mp y) & = \cos x \cos y \pm \sin x \sin y \\
    \sin(x \mp y) & = \sin x \cos y \mp \sin y \cos x \\
\end{aligned}
\end{equation}

\paragraph{Altre identità}
\textit{Formule di Werner}
\begin{equation}
\begin{aligned}
    \cos x \cos y & = \dfrac{1}{2} \left[ \cos(x-y) + \cos(x+y) \right] \\
    \sin x \sin y & = \dfrac{1}{2} \left[ \cos(x-y) - \cos(x+y) \right] \\
    \sin x \cos y & = \dfrac{1}{2} \left[ \sin(x-y) + \sin(x+y) \right] \\
\end{aligned}
\end{equation}

\noindent
\textit{Formule di prostaferesi} Definendo $x-y = p$, $x+y = q$, cosicché $x = \frac{p+q}{2}$, $y = \frac{q-p}{2}$
\begin{equation}
\begin{aligned}
    \cos p + \cos q & = 2 \cos\left(\dfrac{p+q}{2}\right) \cos\left(\dfrac{q-p}{2}\right) \\
    \cos p - \cos q & = 2 \sin\left(\dfrac{p+q}{2}\right) \sin\left(\dfrac{q-p}{2}\right) \\
    \cos p + \cos q & = 2 \sin\left(\dfrac{p+q}{2}\right) \cos\left(\dfrac{q-p}{2}\right) \\
\end{aligned}
\end{equation}



\subsection{Funzioni iperboliche}

% =======================================================================================
\chapter{Limiti di funzioni reali}

% ---------------------------------------------------------------------------------------
\section{Cenni di topologia per l'analisi}
\begin{definition}[Intervallo] Un intervallo è un sottoinsieme di $\mathbb{R}$, l'insieme dei numeri reali.
\end{definition}
\begin{definition}[Estremi superiore e inferiore] Se esiste, l'\textbf{estremo superiore} $M \in \mathbb{R}$ di un intervallo $E$ è il più piccolo maggiorante (cioè un numero uguale o maggiore di tutti i numeri dell'intervallo $E$), cioè:
    \begin{itemize}
        \item $M \ge x$ ,$\forall x \in E$
        \item $\nexists z$ maggiorante di $E$, con $z < M$
    \end{itemize}
    Se esiste, l'\textbf{estremo inferiore} $m \in \mathbb{R}$ di un intervallo $E$ è il più grande minorante (cioè un numero uguale o minore di tutti i numeri dell'intervallo $E$), cioè:
    \begin{itemize}
        \item $m \le x$ ,$\forall x \in E$
        \item $\nexists z$ minorante di $E$, con $z > m$
    \end{itemize}
\end{definition}
\begin{definition}[Intervallo aperto] Un intervallo aperto è un intervallo che non include nessuno dei suoi estremi.
\end{definition}
\begin{definition}[Intervallo chiuso] Un intervallo chiuso è un intervallo che include tutti i suoi estremi.
\end{definition}
Punti: accumulazione/isolato, interni/esterni/frontiera
Intervalli: limitati/illimitati, aperti/chiusi, intorno
\begin{definition}[Intorno di un punto] L'intorno completo $B(x_0, \varepsilon)$ del punto $x_0$ di raggio $\varepsilon$ è l'intervallo aperto
    \begin{equation}
        B(x_0, \varepsilon) = (x_0 - \varepsilon, x_0 + \varepsilon)  
    \end{equation}
\end{definition}
\begin{definition}[Intorno destro e sinisto] L'intorno destro di un punto $x_0$ della retta reale è l'insieme aperto
    \begin{equation}
        B^+(x_0,\varepsilon) = (x_0, x_0+\varepsilon) \ .
    \end{equation}
L'intorno destro di un punto $x_0$ della retta reale è l'insieme aperto
    \begin{equation}
        B^-(x_0,\varepsilon) = (x_0-\varepsilon, x_0) \ .
    \end{equation}
\end{definition}
\begin{definition}[Intorno di infinito]
L'intorno di $-\infty$ viene definito come l'insieme aperto
    \begin{equation}
        B(-\infty,M) = (-\infty,M)
    \end{equation}
L'intorno di $+\infty$ viene definito come l'insieme aperto
    \begin{equation}
        B(+\infty,M) = (M,+\infty)
    \end{equation}
\end{definition}

\begin{definition}[Punto di accumulazione] 
\end{definition}
\begin{definition}[Punto interno]
\end{definition}

\begin{definition}[{\color{red} Intervallo chiuso}] {\color{red} Un intervallo è chiuso se contiene tutti i suoi punti di accumulazione.}
\end{definition}
\begin{definition}[{\color{red} Intervallo aperto}] {\color{red} Un intervallo è aperto se ogni suo punto è un punto interno.}
\end{definition}

\begin{definition}[]
\end{definition}

% ---------------------------------------------------------------------------------------
\section{Funzioni}
\begin{definition}[Funzione a variabile e valori reali]
\end{definition}

% ---------------------------------------------------------------------------------------
\section{Limiti}
{\color{red}
\begin{itemize}
    \item Limite destro e limite sinistro
    \item Funzioni continue
\end{itemize}
}
\begin{definition}[Limite finito al finito] Il limite finito della funzione $f(x)$  per $x$ che tende a $x_0$,
    \begin{equation}
      \lim_{x \rightarrow x_0} f(x) = \ell
    \end{equation}
    è definito dalla condizione
    \begin{equation}
        \text{Per $\forall \varepsilon > 0$ tale che $|x-x_0| < \varepsilon$, $\exists \delta > 0$ tale che $|f(x) - \ell|< \delta$.}
    \end{equation}
\end{definition}
\begin{definition}[Limite infinito al finito] Il limite infinito della funzione $f(x)$  per $x$ che tende a $x_0$,
    \begin{equation}
      \lim_{x \rightarrow x_0} f(x) = \mp \infty
    \end{equation}
    è definito dalla condizione
    \begin{equation}
        \text{Per $\forall \varepsilon > 0$ tale che $|x-x_0| < \varepsilon$, $\exists M \lessgtr 0$ tale che $f(x) \lessgtr M$.}
    \end{equation}
\end{definition}
\begin{definition}[Limite finito al infinito] Il limite finito della funzione $f(x)$ per $x$ che tende all'infinito
    \begin{equation}
      \lim_{x \rightarrow \mp \infty} f(x) = \ell
    \end{equation}
    è definito dalla condizione
    \begin{equation}
        \text{Per $\forall N \lessgtr 0$ tale che $x \lessgtr  N$, $\exists \delta > 0$ tale che $|f(x) - \ell| < \delta$.}
    \end{equation}
\end{definition}
\begin{definition}[Limite infinito al infinito] Il limite infinito della funzione $f(x)$ per $x$ che tende all'infinito
    \begin{equation}
        \lim_{x \rightarrow \mp^{(1)} \infty} f(x) = \mp^{(2)} \infty
    \end{equation}
    è definito dalla condizione
    \begin{equation}
        \text{Per $\forall N \lessgtr^{(1)} 0$ tale che $x \lessgtr^{(1)} N$, $\exists M \lessgtr^{(2)} 0$ tale che $f(x) \lessgtr^{(2)} M$.}
    \end{equation}
\end{definition}

{\color{red}Grafici}

% ---------------------------------------------------------------------------------------
\section{Funzioni continue}
\begin{definition}[Funzione continua in un punto] Una funzione $f: \Omega \rightarrow \mathbb{R}$ è continua in un punto $x_0 \in \Omega$ se 
    \begin{equation}
        \lim_{x \rightarrow x_0}f(x) = f(x_0) \ .
    \end{equation}
\end{definition}
\begin{definition}[Funzione continua in un intervallo] Una funzione $f: \Omega \rightarrow \mathbb{R}$ è continua in un intervallo $[a, b] \subset \Omega$, se è una funzione continua in tutti i punti $x \in [a, b]$.
\end{definition}

\subsection{Teoremi sulle funzioni continue}
\begin{theorem}[Teorema di Weierstrass]\label{thm:weierstrass} Una funzione continua definita su un intervallo chiuso e limitato $[a, b]$ ammette in esso un massimo e un minimo assoluto.
\end{theorem}

\begin{theorem}[Teorema della permanenza del segno]\label{thm:sign} Preso un punto $x_0$ appartenente al dominio $\Omega$ di un funzione continua, $f: \Omega \rightarrow \mathbb{R}$, tale che $f(x_0)>0$, allora esiste un intorno $B(x_0)$ del punto $x_0$ tale che $f(x) > 0$ per $\forall x \in B(x_0)$.
\end{theorem}

\begin{theorem}[Teorema dei valori intermedi]\label{thm:intermediate-values} Data una funzione continua $f: [a,b] \rightarrow \mathbb{R}$, la funzione assume nell'intervallo $[a,b]$ tutti i valori compresi tra $f(a)$ e $f(b)$.

    \noindent
    Assumendo $f(a) < f(b)$, per ogni valore $y_0 \in [f(a), f(b)]$ esiste almeno un valore $x_0 \in [a,b]$ tale che $y_0 = f(x_0)$.
\end{theorem}

% ---------------------------------------------------------------------------------------
\section{Teoremi sui limiti}
\begin{theorem}[Teorema del confronto]\label{thm:comparison}
\end{theorem}

\begin{theorem}[Teorema di de l'Hopital -- anticipazione]\label{thm:hopital:0} Il teorema di de l'Hopital riguarda il calcolo delle forme indeterminate $\frac{0}{0}$, $\frac{\infty}{\infty}$, cioè i limiti che possono essere scritti nella forma
    \begin{equation}
        \lim_{x \rightarrow x_0} \dfrac{f(x)}{g(x)}
    \end{equation}
    con
    \begin{itemize}
        \item $\lim_{x \rightarrow x_0} f(x) = \lim_{x \rightarrow x_0} g(x) = 0$ oppure
        \item $\lim_{x \rightarrow x_0} |f(x)| = \lim_{x \rightarrow x_0} |g(x)| = +\infty$
    \end{itemize}
\end{theorem}
    Per l'enunciato completo e la dimostrazione del teorema di de l'Hopital si rimanda al teorema \S\ref{thm:hopital} cap.\S\ref{ch:derivatives} sulle derivate.

% ---------------------------------------------------------------------------------------
\section{Infiniti e infinitesimi}

% ---------------------------------------------------------------------------------------
\section{Limiti notevoli}
\begin{center}
\begin{tabular}{ll}
    Potenza            & $\displaystyle\lim_{x \rightarrow 0} \dfrac{(1+x)^a - 1}{x} = a$ \\
    Seno               & $\displaystyle\lim_{x \rightarrow 0} \dfrac{\sin}{x} = 1$ \\
    Coseno             & $\displaystyle\lim_{x \rightarrow 0} \dfrac{1 - \cos x}{x^2} = \frac{1}{2}$ \\
    Esponenziale - def & $\displaystyle\lim_{x \rightarrow \infty} \left( 1 + \dfrac{1}{x} \right)^x = e$ \\
    \qquad " \qquad (1)& $\displaystyle\lim_{x \rightarrow 0} \dfrac{e^x - 1}{x} = 1$ \\
    \qquad " \qquad (2)& $\displaystyle\lim_{x \rightarrow 0} \dfrac{e^{x}}{1+x} = 1$ \\
    Logaritmo naturale & $\displaystyle\lim_{x \rightarrow 0} \dfrac{\ln (1+x)}{x} = 1$ \\
\end{tabular}
\end{center}

\subsection{Dimostrazioni}
\begin{proof}[Limite notevole del seno] Il limite viene calcolato usando il teorema del confronto (\ref{thm:comparison}), con le funzioni $x$, $\sin x$ e $\tan x$
    \begin{equation}
        \sin x \le x \le \tan x \qquad \rightarrow \qquad 1 \le \dfrac{x}{\sin{x}} \le \dfrac{1}{\cos x} \ .
    \end{equation}
    Per $x \rightarrow 0$, $\displaystyle\lim_{x \rightarrow 0} 1 = 1$ e $\displaystyle\lim_{x \rightarrow 0} \frac{1}{\cos x} = 1$ e quindi $ \lim_{x \rightarrow 0} \dfrac{ x }{\sin x} = 1 $.
\end{proof}
\begin{proof}[Limite notevole del coseno] Il limite viene calcolato usando il limite notevole del seno,
    \begin{equation}
        \begin{aligned}
            \lim_{x \rightarrow 0} \dfrac{1 - \cos x}{x^2} 
            & = \lim_{x \rightarrow 0} \dfrac{1 - \cos x}{x^2} \dfrac{1+\cos x}{1+\cos x} = \\
            & = \lim_{x \rightarrow 0} \dfrac{1 - \cos^2 x}{x^2} \dfrac{1}{1+\cos x} = \\ 
            & = \lim_{x \rightarrow 0} \underbrace{\dfrac{\sin^2 x}{x^2}}_{\left( \frac{\sin x}{x} \right)^2 \rightarrow 1} \underbrace{\dfrac{1}{1+\cos x}}_{\rightarrow \frac{1}{2}} = \frac{1}{2}
        \end{aligned}
    \end{equation}
\end{proof}

{\color{red}
\noindent
REMOVE from here, MOVE to chapter about series
\begin{definition}[Il numero di Nepero, $e$] Il numero di Nepero, $e$ può essere definito come il limite
    \begin{equation}
        e := \lim_{n \rightarrow \infty} \left( 1 + \dfrac{1}{n} \right)^n \ .
    \end{equation}
\end{definition}
Si prova l'esistenza al finito di questo limite, dimostrando che la serie $a_n := \left( 1 + \frac{1}{n} \right)^n $ è:
\begin{itemize}
    \item crescente in maniera monotona, i.e. $a_{n+1} > a_n$, $\forall n$
    \item limitata superiormente, i.e. $\exists M s.t. a_n < M$, $\forall n$
\end{itemize}
Per dimostrare che la serie è crescente, cerchiamo il legame tra due termini successivi.
Per dimostrare che la serie è limitata superiormente, dimostriamo che $a_n < 3$.
}

\begin{proof}[Limite notevole dell'esponenziale - definizione] Questo limite non è nient'altro che una delle definizioni del numero $e$, come mostrato nella Sez.\S\ref{sec:napier}.
\end{proof}
\begin{proof}[Limite notevole dell'esponenziale - (1)]
    \begin{equation}
        \begin{aligned}
            \lim_{x \rightarrow 0} \dfrac{e^x - 1}{x} 
            & = \lim_{x \rightarrow 0} \dfrac{1}{x}\sum_{k=0}^{\infty} \left( \dfrac{x^k}{k!} - 1 \right) = \\
            & = \lim_{x \rightarrow 0} \dfrac{1}{x}\sum_{k=1}^{\infty} \dfrac{x^k}{k!} = \\
            & = \lim_{x \rightarrow 0} \dfrac{1}{x} \left( x + \dfrac{x^2}{2} + \dfrac{x^3}{3!} + \dots  \right) = \\
            & = \lim_{x \rightarrow 0} \left( 1 + \dfrac{x}{2} + \dfrac{x^2}{3!} + \dots  \right) = 1 
        \end{aligned}
    \end{equation}
\end{proof}
\begin{proof}[Limite notevole dell'esponenziale - (2)]
    \begin{equation}
        \begin{aligned}
            \lim_{x \rightarrow 0} \dfrac{e^x}{1+x} 
            & = \lim_{x \rightarrow 0} \dfrac{1}{1+x}\sum_{k=0}^{\infty} \dfrac{x^k}{k!} = \\
            & = \lim_{x \rightarrow 0} \dfrac{1}{1+x}\left( 1 + x + \dfrac{x^2}{2} + \dfrac{x^3}{3!} + \dots \right) = \\
            & = \lim_{x \rightarrow 0} \bigg[ 1 + \underbrace{\dfrac{x^2}{1+x} \left( \dfrac{1}{2} + \dfrac{x}{3!} + \dots \right)}_{\rightarrow 0} \bigg] = 1 \\
        \end{aligned}
    \end{equation}
\end{proof}
\begin{proof}[Limite notevole del logaritmo]
    \begin{equation}
        \begin{aligned}
            \lim_{x \rightarrow 0} \dfrac{e^x}{1+x} 
            & = \lim_{x \rightarrow 0} \dfrac{1}{1+x}\sum_{k=0}^{\infty} \dfrac{x^k}{k!} = \\
            & = \lim_{x \rightarrow 0} \dfrac{1}{1+x}\left( 1 + x + \dfrac{x^2}{2} + \dfrac{x^3}{3!} + \dots \right) = \\
            & = \lim_{x \rightarrow 0} \bigg[ 1 + \underbrace{\dfrac{x^2}{1+x} \left( \dfrac{1}{2} + \dfrac{x}{3!} + \dots \right)}_{\rightarrow 0} \bigg] = 1 \\
        \end{aligned}
    \end{equation}
\end{proof}

% =======================================================================================
\chapter{Derivate}\label{ch:derivatives}

% ---------------------------------------------------------------------------------------
\section{Definizioni}
\begin{definition}[Rapporto incrementale] Il rapporto incrementale della funzione reale a valori reali $f: \mathbb{R} \rightarrow \mathbb{R}$ di un incremento $\Delta x$ attorno al valore $x$ della variabile indipendente è definito come il rapporto della differenza del valore della funzione e del valore dell'incremento,
\begin{equation}
    \Delta f (x; \Delta x) := \dfrac{f(x+\Delta x) - f(x)}{\Delta x} \ .
\end{equation}
\end{definition}

\begin{definition}[Derivata]\label{def:derivative}
La derivata della funzione reale a valori reali $f: \mathbb{R} \rightarrow \mathbb{R}$ è una funzione reale a valori reali, denotata $\frac{d f}{dx}$, o $f'(x)$, e definita in ogni punto come il limite del rapporto incrementale per valori dell'incremento che tendono a $0$,
\begin{equation}
  f'(x) = \dfrac{d }{dx} f(x) := \lim_{\Delta x \rightarrow 0} \dfrac{f(x+\Delta x) - f(x)}{\Delta x} \ .
\end{equation}
\end{definition}

\begin{definition}[Funzione derivabile]\label{def:differentiable-fcn} Una funzione $f: \Omega \in \mathbb{R} \rightarrow \mathbb{R}$ è \textbf{derivabile in un punto} $x$ se esiste il limite del rapporto incrementale, $\Delta f[x, \Delta x]$. La funzione è \textbf{derivabile in un intervallo} $I$, se è derivabile in tutti per tutti i valori di $x$ compresi nell'intervallo $I$.
\end{definition}

\paragraph{Interpretazione geometrica.}
La derivata della funzione $f(x)$ ha un'interpretazione geometrica immediata, se si rappresenta nel piano cartesiano il grafico della funzione, $y = f(x)$. Il valore della derivata $f'(x)$ coincide con la pendenza della retta tangente al grafico della funzione $y = f(x)$, nel punto $(x,y) = (x, f(x))$.

{\color{red}
\begin{itemize}
    \item relazione con la secante
    \item grafico (.gif?) per valore puntuale, con limite della secante alla tangente
    \item grafico (.gif?) per funzione
\end{itemize}
}

% ---------------------------------------------------------------------------------------
\section{Regole di derivazione}
Usando la definizione di derivata (\ref{def:derivative}) si possono dimostrare le seguenti regole per il calcolo delle derivate.
\paragraph{Derivata della somma di due funzioni e il prodotto per uno scalare}
\begin{equation}
\begin{aligned}
    (f(x) + g(x))' & = f'(x) + g'(x) \\
    (a f(x))' & = a f'(x) \\
\end{aligned}
\end{equation}
\begin{property}[Operatore lineare] La derivata è un operatore lineare.
\end{property}

\paragraph{Derivata del prodotto di due funzioni}
\begin{equation}
    (f(x) g(x))' = f'(x) g(x) + f(x) g'(x)
\end{equation}
\paragraph{Derivata del rapporto di due funzioni}
\begin{equation}
    \left( \dfrac{f(x)}{g(x)} \right)' = \dfrac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}
\end{equation}
\paragraph{Derivata di una funzione composta}
\begin{equation}
    \dfrac{d}{dx} f(g(x)) = \dfrac{d}{dy} f(y) \bigg|_{y=g(x)} \dfrac{d}{dx} g(x)
\end{equation}
\paragraph{Derivata della funzione inversa}
\begin{equation}
    \dfrac{d}{dx} f^{-1}(x) = \dfrac{1}{\dfrac{d f}{d y}\bigg|_{y=f(x)}}
\end{equation}

\subsection{Dimostrazioni}

\begin{proof}[Derivata della combinazione lineare di due funzioni]
{\small
\begin{equation}
\begin{aligned}
    \dfrac{d}{dx} \left( \alpha f(x) + \beta g(x) \right) & = 
    \lim_{\Delta x \rightarrow 0} \dfrac{\alpha f(x+\Delta x) + \beta g(x+\Delta x) - \left[ \alpha f(x) + \beta g(x) \right] }{\Delta x} =  \\
    & = \lim_{\Delta x \rightarrow 0} \left[ \alpha \dfrac{f(x+\Delta x) - f(x)}{\Delta x} + \beta \dfrac{g(x+\Delta x) - g(x) }{\Delta x} \right] =  \\
    & = \alpha \lim_{\Delta x \rightarrow 0} \dfrac{f(x+\Delta x) - f(x)}{\Delta x} + \beta \lim_{\Delta x \rightarrow 0} \dfrac{g(x+\Delta x) - g(x) }{\Delta x} =  \\
    & = \alpha f'(x) + \beta g'(x) \ .
\end{aligned}
\end{equation}
}
\end{proof}
\begin{proof}[Derivata del prodotto di due funzioni]
{\small
\begin{equation}
\begin{aligned}
\dfrac{d}{dx} \left(f(x) g(x) \right) & =
     \lim_{\Delta x \rightarrow 0} \dfrac{f(x+\Delta x) g(x+\Delta x) - f(x)g(x) }{\Delta x} =  \\
 & = \lim_{\Delta x \rightarrow 0} \dfrac{f(x+\Delta x) g(x+\Delta x) - f(x+\Delta x) g(x) + f(x+\Delta x) g(x) - f(x)g(x) }{\Delta x} =  \\
 & = \lim_{\Delta x \rightarrow 0} \dfrac{f(x+\Delta x) g(x+\Delta x) - f(x+\Delta x) g(x) + f(x+\Delta x) g(x) - f(x)g(x) }{\Delta x} =  \\
    & = \lim_{\Delta x \rightarrow 0} f(x+\Delta x) \dfrac{ g(x+\Delta x) - g(x)}{\Delta x} + \lim_{\Delta x \rightarrow 0} \dfrac{ f(x+\Delta x)  - f(x) }{\Delta x} g(x) = \\
    & = f'(x)g(x) + f(x)g'(x) \ .
\end{aligned}
\end{equation}
}
\end{proof}
\begin{proof}[Derivata del rapporto di due funzioni]
{\small
\begin{equation}
\begin{aligned}
    \dfrac{d}{dx} \left(\dfrac{f(x)}{ g(x)} \right) & =
     \lim_{\Delta x \rightarrow 0} \dfrac{1}{\Delta x} \left[ \dfrac{f(x+\Delta x)}{ g(x+\Delta x)} - \dfrac{f(x)}{g(x)} \right] =  \\
    & = \lim_{\Delta x \rightarrow 0} \dfrac{1}{\Delta x} \dfrac{f(x+\Delta x) g(x) - f(x)g(x+\Delta x)}{g(x+\Delta x)g(x)}  =  \\
    & = \lim_{\Delta x \rightarrow 0} \dfrac{1}{g(x+\Delta x)g(x)} \dfrac{f(x+\Delta x) g(x) - f(x) g(x) + f(x)g(x)  - f(x)g(x+\Delta x)}{\Delta x}  =  \\
    & = \lim_{\Delta x \rightarrow 0} \dfrac{1}{g(x+\Delta x)g(x)} \dfrac{f(x+\Delta x) - f(x)}{\Delta x} g(x) - \lim_{\Delta x \rightarrow 0} \dfrac{1}{g(x+\Delta x)g(x)} \dfrac{g(x+\Delta x) - g(x)}{\Delta x} f(x) =  \\
    & = \dfrac{f'(x) g(x)}{g^2(x)} - \dfrac{f(x)g'(x)}{g^2(x)} = \dfrac{f'(x)g(x) - f(x) g'(x)}{g^2(x)}
\end{aligned}
\end{equation}
}
\end{proof}
\begin{proof}[Derivata di una funzione composta]
\begin{equation}
\begin{aligned}
    \dfrac{d}{dx} f(g(x)) & =
     \lim_{\Delta x \rightarrow 0} \dfrac{1}{\Delta x} \left[ f(g(x+\Delta x)) - f(g(x)) \right] =  \\
    & = \lim_{\Delta x \rightarrow 0} \dfrac{f(g(x+\Delta x)) - f(g(x))}{g(x+\Delta x) - g(x)} \dfrac{g(x+\Delta x) - g(x)}{\Delta x} =  \\
    & = \dots \\
    & = f'(g(x)) \, g'(x) \ .
\end{aligned}
\end{equation}
\end{proof}

\begin{proof}[Derivata della funzione inversa] Per dimostrare la formula della derivata della funzione inversa $f^{-1}(x)$ della funzione $f(x)$, si sfrutta la definizione
\begin{equation}
    f(y) = x \quad , \quad f^{-1}(x) = y \qquad \rightarrow \qquad x = f(f^{-1}(x)) \ .
\end{equation}
Si calcola la derivata rispetto a $x$ dell'ultima relazione, usando la regola di derivazione delle funzioni composte
\begin{equation}
    \frac{d}{dx} x = \frac{d}{dx} \left( f(f^{-1}(x)) \right) \qquad \rightarrow \qquad
    1 = \dfrac{d f}{d y}\bigg|_{y=f(x)} \dfrac{d f^{-1}(x)}{dx} 
\end{equation}
\begin{equation}
    \rightarrow \qquad \dfrac{d}{dx} f^{-1}(x) = \dfrac{1}{\dfrac{d f}{d y}\bigg|_{y=f(x)}}
\end{equation}
\end{proof}

% ---------------------------------------------------------------------------------------
\section{Teoremi}
\begin{theorem}[Teorema di Fermat]\label{thm:fermat} Sia $f: \ (a,b) \rightarrow \mathbb{R}$ una funzione definita sull'intervallo $(a,b)$ e derivabile nel punto $x_0 \in (a,b)$, punto di estremo locale. Allora $f'(x_0) = 0$.
\end{theorem}

\begin{theorem}[Teorema di Rolle]\label{thm:rolle} Sia $f: \ [a,b] \rightarrow \mathbb{R}$ una funzione continua e derivabile in ogni punto dell'intervallo $[a,b]$ che assume valori uguali $f(a) = f(b)$, allora esiste almeno un punto $c \in (a,b)$ in cui la derivata della funzione si annulla, $f'(c) = 0$.
\end{theorem}
\begin{proof}
    Secondo il teorema di Weierstrass (\ref{thm:weierstrass}) sulle funzioni continue, la funzione $f$ ammette punti $c \in [a,b]$ di massimo e un minimo globali. Si possono riconoscere ora due casi:
    \begin{itemize}
        \item se i punti di massimo e minimo globali coincidono entrambi con gli estremi dell'intervallo, si può scrivere
            \begin{equation} m = f(a) = f(b) \le f(x) \le M = f(a) = f(b) \end{equation}
                e quindi la funzione è costante, $f(x) = f(a) = f(b)$, e la sua derivata è nulla in tutti i punti dell'intervallo,$f'(x) = 0, \ \forall x \in (a,b)$;
            \item altrimenti, esiste un punto di massimo o minimo globale $x=c \in (a,b)$ interno all'intervallo; i punti di massimo o minimo globali globali sono anche punti di massimo o minimo locali e quindi, per il teorema di Fermat (\ref{thm:fermat}), $f'(c) = 0$.
    \end{itemize} 
\end{proof}

\begin{theorem}[Teorema di Cauchy]\label{thm:cauchy} Siano $f, g: \ [a,b] \rightarrow \mathbb{R}$ due funzioni continue in $[a,b]$ e derivabili su $(a,b)$, allora esiste un punto $c \in (a,b)$ tale che
    \begin{equation}
        \left[ f(b) - f(a)  \right]g'(c) = \left[ g(b) - g(a)  \right]f'(c) \ .
    \end{equation}
\end{theorem}
\begin{proof}
  La dimostrazione del teorema di Cauchy può essere svolta con un'opportuna scelta della funzione alla quale applicare il teorema di Rolle (\ref{thm:rolle}). Ad esempio, la funzione
    \begin{equation}
       h(x) = [f(b) - f(a)] [ g(x) - g(a)] - [g(b)-g(a)] [f(x) - f(a)] \ ,
    \end{equation}
    soddisfa le ipotesi del teoremda di Rolle, poichè
    \begin{itemize}
        \item $h(x)$ è continua su $[a,b]$ e derivabile su $(a,b)$
        \item $h(a) = h(b)$, dal calcolo diretto
            \begin{equation}
                \begin{aligned}
                    h(a) & = [f(b) - f(a)] [ g(a) - g(a)] - [g(b)-g(a)] [f(a) - f(a)] = 0 \\ 
                    h(b) & = [f(b) - f(a)] [ g(b) - g(a)] - [g(b)-g(a)] [f(b) - f(a)] = 0  \ .
                \end{aligned}
            \end{equation}
    \end{itemize}
    Applicando il teorema di Rolle alla funzione $h(x)$, possiamo concludere che esiste un punto $c \in (a,b)$ tale che $h'(c) = 0$ e quindi
    \begin{equation}
        0 = h'(c) = [f(b) - f(a)] g'(x) - [g(b)-g(a)] f'(x) \ .
    \end{equation}
\end{proof}

\begin{theorem}[Teorema di Lagrange]\label{thm:lagrange} Sia $f: \ [a,b] \rightarrow \mathbb{R}$ una funzione continua in $[a,b]$ e derivabile in $(a,b)$, allora esiste un punto $c \in (a,b)$ tale che
    \begin{equation}
        f(b) - f(a) = f'(c) (b-a) \ .
    \end{equation}
\end{theorem}
\begin{proof}
    La dimostrazione del teorema di Lagrange segue direttamente dalla dimostrazione del teorema di Cauchy (\ref{thm:cauchy}), usando le funzioni $f(x)$ e $g(x) = x$, la cui derivata è $g'(x) = 1$.
\end{proof}

\subsection{Teorema di de l'Hopital}
\begin{theorem}[Teorema di de l'Hopital]\label{thm:hopital}
\end{theorem}

% ------------------------------------------------------------------------------
\section{Derivate fondamentali}
\begin{center}
    \begin{tabular}{lll}
        & \textbf{funzione} & \textbf{derivata} \\
        Potenza      & $f(x) = x^a$       & $f'(x) = a x^{a-1}$    \\
        Esponenziale & $f(x) = e^x$       & $f'(x) = e^x$          \\
        Logaritmo    & $f(x) = \ln x$     & $f'(x) = \frac{1}{x}$  \\
        Seno         & $f(x) = \sin x$    & $f'(x) = \cos x$       \\
        Coseno       & $f(x) = \cos x$    & $f'(x) =-\sin x$       \\
    \end{tabular}
\end{center}
\subsection{Dimostrazioni}
\begin{proof}[Derivata della potenza]
\begin{equation}
    \begin{aligned}
    (x^n)'
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{(x+\varepsilon)^n - x^n}{\varepsilon} = \\
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{ \left( 1 + \frac{\varepsilon}{x} \right)^n - 1 }{\varepsilon} x^n = \\
        & = \underbrace{\lim_{\varepsilon \rightarrow 0 } \dfrac{ \left( 1 + \frac{\varepsilon}{x} \right)^n - 1 }{\frac{\varepsilon}{x}}}_{=n} x^{n-1} = \\
        & = n x^{n-1}
    \end{aligned}
\end{equation}
\end{proof}

\begin{proof}[Derivata della funzione esponenziale]
\begin{equation}
    \begin{aligned}
        (e^x)' 
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{e^{x+\varepsilon} - e^x}{\varepsilon} = \\
        & = \underbrace{\lim_{\varepsilon \rightarrow 0 } \dfrac{(e^{\varepsilon} - 1)}{\varepsilon}}_{= 1} e^{x} = \\
        & = e^x
    \end{aligned}
\end{equation}
\end{proof}
\begin{proof}[Derivata della funzione logaritmo] {\color{red} Il dominio della funzione logaritmo è $\mathbb{R}^+$, ossia l'insieme dei numeri reali positivi. Essendo l'argomento del logaritmo diverso da zero, si può dividere per esso senza il timore di incorrere in espressioni non definite}
\begin{equation}
    \begin{aligned}
        (\ln x)' 
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{\ln(x+\varepsilon) - \ln x}{\varepsilon} = \\
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{\ln \left( x\left( 1 + \frac{\varepsilon}{x} \right) \right) - \ln x}{\varepsilon} = \\
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{\ln  x + \ln \left( 1 + \frac{\varepsilon}{x}  \right) - \ln x}{\varepsilon} = \\
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{ \ln \left( 1 + \frac{\varepsilon}{x}  \right) }{\varepsilon} = \\
        & = \underbrace{\lim_{ \small\begin{matrix} \varepsilon \rightarrow 0 \\  \varepsilon / x \rightarrow 0  \end{matrix}} \dfrac{ \ln \left( 1 + \frac{\varepsilon}{x}  \right) }{\frac{\varepsilon}{x}}}_{=1} \dfrac{1}{x} = \\
        & = \dfrac{1}{x}
    \end{aligned}
\end{equation}
\end{proof}
\begin{proof}[Derivata della funzione seno]
\begin{equation}
    \begin{aligned}
        (\sin x)' 
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{\sin(x+\varepsilon) - \sin x}{\varepsilon} = \\
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{\sin x \cos\varepsilon + \sin \varepsilon \cos x - \sin x}{\varepsilon} = \\
        & = \underbrace{\lim_{\varepsilon \rightarrow 0 } \left( \dfrac{ \cos\varepsilon - 1}{\varepsilon} \right) }_{\lim_{\varepsilon \rightarrow 0} \left(-\frac{1}{2} \varepsilon \right) = 0 } \sin x +  \underbrace{\lim_{\varepsilon \rightarrow 0 } \dfrac{\sin \varepsilon}{\varepsilon} }_{= 1} \cos x  = \\
        & = \cos x 
    \end{aligned}
\end{equation}
\end{proof}
\begin{proof}[Derivata della funzione coseno]
\begin{equation}
    \begin{aligned}
        (\cos x)' 
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{\cos(x+\varepsilon) - \cos x}{\varepsilon} = \\
        & = \lim_{\varepsilon \rightarrow 0 } \dfrac{\cos x \cos\varepsilon - \sin \varepsilon \sin x - \cos x}{\varepsilon} = \\
        & = \underbrace{\lim_{\varepsilon \rightarrow 0 } \left( \dfrac{ \cos\varepsilon - 1}{\varepsilon} \right) }_{\lim_{\varepsilon \rightarrow 0} \left(-\frac{1}{2} \varepsilon \right) = 0 } \cos x -  \underbrace{\lim_{\varepsilon \rightarrow 0 } \dfrac{\sin \varepsilon}{\varepsilon} }_{= 1} \sin x  = \\
        & = - \sin x 
    \end{aligned}
\end{equation}
\end{proof}

% ------------------------------------------------------------------------------
\section{Derivate di ordine superiore}
Poiché la derivata $f'(x)$ di una funzione $f: \mathbb{R} \rightarrow \mathbb{R}$ è una funzione $f': \mathbb{R} \rightarrow \mathbb{R}$, si può calcolare la derivata della funzione $f'(x)$

% ------------------------------------------------------------------------------
\section{Espansioni in serie di Taylor e MacLaurin}\label{ch:taylor}
\begin{definition}[Serie di Taylor] La serie di Taylor di una funzione $f(x)$ centrata in $x=x_0$ è la serie polinomiale
\begin{equation}
   T[f(x_0)](x) := \sum_{n=0}^{\infty} \dfrac{f^{(n)}(x_0)}{n!} (x-x_0)^n \ .
\end{equation}
\end{definition}
%
\begin{theorem}
La serie di Taylor troncata alla $n$-esima potenza,
\begin{equation}
  T^n[f(x_0)](x) = \sum_{k=0}^{n} \dfrac{f^{(k)}(x_0)}{i!} (x-x_0)^k \ ,
\end{equation}
è un'approssimazione dell'$n$-esimo ordine della funzione $f(x)$, i.e.
\begin{equation}
  f(x) - T^n[f(x_0)](x) \sim o(|x-x_0|^{n})
\end{equation}
\end{theorem}

\begin{definition}[Serie di MacLaurin] La serie di MacLaurin di una funzione $f(x)$ è definita come la sua serie di Taylor centrata in $x=0$.
\end{definition}

% ------------------------------------------------------------------------------
\section{Applicazioni}
\subsection{Studio funzione}
\subsubsection{Punti in cui $f'(x) = 0$: punti di estremo locale e flessi orizzontali}
Per una funzione derivabile, i punti in cui si annulla la derivata prima di una funzione $f(x)$ possono essere:
\begin{itemize}
    \item punti di massimo locale
    \item punti di minimo locale
    \item punti di flesso orizzontale
\end{itemize}
\begin{definition}[Punto di massimo locale]
\end{definition}
\begin{theorem}[Condizioni per un punto di massimo locale] Il punto $x_0$ è un punto di massimo locale per una funzione $f(x)$ derivabile se
    \begin{itemize}
        \item $f'(x_0) = 0$
        \item la prima derivata di ordine superiore valutata in $x_0$ diversa da zero, è una derivata di \textbf{ordine pari} ed è \textbf{negativa}, $f^{(n)}(x_0) < 0$
    \end{itemize}
\end{theorem}
\begin{definition}[Punto di minimo locale]
\end{definition}
\begin{theorem}[Condizioni per un punto di minimo locale] Il punto $x_0$ è un punto di mimimo locale per una funzione $f(x)$ derivabile se
    \begin{itemize}
        \item $f'(x_0) = 0$
        \item la prima derivata di ordine superiore valutata in $x_0$ diversa da zero, è una derivata di \textbf{ordine pari}, ed è \textbf{positiva}, $f^{(n)}(x_0) > 0$
    \end{itemize}
\end{theorem}
\begin{definition}[Punto di flesso orizzontale]
\end{definition}
\begin{theorem}[Condizioni per un punto di flesso orizzontale] Il punto $x_0$ è un punto di flesso orizzontale per una funzione $f(x)$ derivabile se
    \begin{itemize}
        \item $f'(x_0) = 0$
        \item la prima derivata di ordine superiore valutata in $x_0$ diversa da zero, è una derivata di \textbf{ordine dispari}
    \end{itemize}
\end{theorem}

\subsubsection{Punti in cui $f''(x) = 0$: punti di flesso}

\begin{example}[Studio di funzione] Si vuole studiare la funzione $f(x) = \frac{e^x}{x-1}$
    \newline\textbf{Dominio.}
    \newline\textbf{Limiti all'infinito e agli eventuali punti di discontinuità.}
    \newline\textbf{Ricerca minimi e massimi locali}
\end{example}

\subsection{Ottimizzazione libera e vincolata}
Il tipico problema di ottimizzazione consiste nella ricerca di un massimo o un minimo di una funzione,
\begin{equation}
    \mathbf{x}^* = \underset{\mathbf{x}}{\text{argmax}} \, f(\mathbf{x}) \ .
\end{equation}
Questa ricerca può essere soggetta a vincoli sulla variabile indipendente, che in generale possono essere:
\begin{itemize}
    \item valori estremi della variabile
    \item vincoli di uguaglianza della forma $\mathbf{g}(\mathbf{x}) = \mathbf{0}$
    \item vincoli di disuguaglianza della forma $\mathbf{h}(\mathbf{x}) \ge \mathbf{0}$
\end{itemize}

\subsubsection{Ottimizzazione libera}

\subsubsection{Ottimizzazione vincolata}

\paragraph{Medoto dei moltiplicatori di Lagrange per vincoli di uguaglianza}



% =======================================================================================
\chapter{Integrali}\label{ch:integrals}

\section{Definizioni}
\begin{definition}[Somma di Riemann] Data una funzione continua $f:[a,b] \rightarrow \mathbb{R}$, e una partizione $\mathcal{P} = \left\{x_0, x_1, \dots, x_n | a = x_0 < x_1 < \dots x_n = b \right\}$ dell'intervallo $[a,b]$, la somma di Riemann viene definita come
    \begin{equation}
        \sigma_n = \sum_{i=1}^{n} f(\xi_i) \ (x_{i} - x_{i-1})
    \end{equation}
con $\xi_i \in [x_{i-1}, x_i]$.
\end{definition}

\begin{definition}[Integrale di Riemann]\label{def:riemann-integral} Definendo $\Delta x := \max_i(x_i - x_{i-1}) $, l'integrale di Riemann viene definito come il limite della somma di Riemann per $\Delta x  \rightarrow 0$ (e di conseguenza il numero di intervalli della partizione $n \rightarrow \infty$), e viene indicato come
    \begin{equation}
        \int_{x=a}^b f(x) dx = \lim_{\Delta x \rightarrow 0} \sigma_n
    \end{equation}
\end{definition}

\begin{definition}[Integrale definito] L'integrale della definizione (\ref{def:riemann-integral}) svolto tra due valori di $x$ viene definito integrale definito. 
\end{definition}

\paragraph{Interpretazione geometrica}

\paragraph{Proprietà degli integrali definiti}
\begin{equation}\label{prop:integral:linearity:1}
    \int_{x=a}^{b} \left[ A f(x) + B g(x) \right] dx = A \int_{x=a}^{b} f(x) dx + B \int_{x=a}^{b} g(x) dx
\end{equation}
\begin{equation}\label{prop:integral:linearity:2}
    \int_{x=a}^{b} f(x) dx = \int_{x=a}^{c} f(x) dx + \int_{x=c}^{b} f(x) dx
\end{equation}

\begin{definition}[Integrale indefinito] Si può definire l'integrale indefinito di una funzione come l'integrale definito funzione del secondo estremo dell'intervallo, con il primo estremo fissato, 
    \begin{equation}
        \int_{t=a}^{x} f(t) \, dt = F(x) + c \ ,
    \end{equation}
\end{definition}

% ------------------------------------------------------------------------------
\section{Teoremi}
\begin{definition}[Media]
    \begin{equation}
        M(f, [a,b]) = \dfrac{1}{b-a} \int_{x=a}^{b} f(x) dx
    \end{equation}
\end{definition}

\begin{theorem}[Teorema della media]\label{thm:integral:average}
    Sia $f: \ [a,b] \rightarrow \mathbb{R}$ una funzione continua su $[a,b]$, allora esiste un punto $c \in [a,b]$ tale che
    \begin{equation}
        \int_{x=a}^{b} f(x) dx = f(c) (b-a) \ .
    \end{equation}
\end{theorem}
\begin{proof}
    Poiché $f(x)$ è continua per $x \in [a,b]$, allora per il teorema di Weierstrass (\ref{thm:weierstrass}) si può scrivere
    \begin{equation}
        m \le f(x) \le M \ ,
    \end{equation}
    essendo $m$, $M$ i punti di minimo e massimo della funzione. Integrando questa relazione su $[a,b]$, si ottiene
    \begin{equation}
    \begin{aligned}
        & \int_{a}^{b} m \, dx \le \int_{a}^{b} f(x) \, dx \le \int_{a}^{b} M \, dx \\ 
        & m \, (b-a) \le \int_{a}^{b} f(x) \, dx \le M \, (b-a) \\ 
        & m  \le \dfrac{1}{b-a} \int_{a}^{b} f(x) \, dx \le M  \ .
    \end{aligned}
    \end{equation}
    Chiamando $a$, $b$ i valori per cui $f(a) = m$,  $f(b) = M$, per il teorema dei valori intermedi (\ref{thm:intermediate-values}), per ogni valore $y$ della funzione compreso tra $f(a)$ e $f(b)$, esiste almeno un punto $c \in [a,b]$ tale che $y = f(c)$.
    Tra tutti i valori compresi tra gli estremi, questo numero deve esistere anche per il valore particolare (compreso tra gli estremi) $y = \frac{1}{b-a} \int_{a}^{b} f(x) \, dx$, e quindi
    \begin{equation}
       f(c) = \frac{1}{b-a} \int_{a}^{b} f(x) \, dx \ .
    \end{equation}
    Moltiplicando per il fattore $(b-a)$ si conclude la dimostrazione ottenendo l'espressione dell'enunciato del teorema.
%    {\color{red} \textbf{dimostrazione non valida, poiché usa il risultato del thm fondamentale del calcolo integrale}
%    La dimostrazione è immediata, applicando il teorema di Lagrange (\ref{thm:lagrange}) alla primitiva $F(x)$ della funzione $f(x)$, $F(x) = \int_{t=x_0}^{x} f(t) dt$, 
%\begin{equation}
%\begin{aligned}
%    \int_{t=a}^{b} f(x) dx & = F(b) - F(a) = & \text{(teorema di Lagrange (\ref{thm:lagrange}): $c \in [a,b]$)} \\
%    & = F'(c) \, (b-a) = f(c) \, (b-a) \ .
%\end{aligned}
%\end{equation}
%    }
\end{proof}


\begin{theorem}[Teorema fondamentale del calcolo infinitesimale]\label{thm:fundamental-calculus}
    \begin{equation}
        \dfrac{d}{dx} \int_{t=a}^{x} f(t) dt = f(x)
    \end{equation}
\end{theorem}
\begin{proof}
\begin{equation}
    \begin{aligned}
        & \dfrac{d}{dx} \int_{t=a}^{x} f(t) dt = & \text{(def. di derivata)} \\ 
        & = \lim_{\Delta x \rightarrow 0} \dfrac{1}{\Delta x} \left[ \int_{t=a}^{x+\Delta x} f(t) dt - \int_{t=a}^{x} f(t) dt\right] = & \text{(prorietà (\ref{prop:integral:linearity:2}))} \\
        & = \lim_{\Delta x \rightarrow 0} \dfrac{1}{\Delta x} \int_{t=x}^{x+\Delta x} f(t) dt = & \text{(teorema della media (\ref{thm:integral:average}))} \\
        & = \lim_{\Delta x \rightarrow 0} \underbrace{\dfrac{1}{\Delta x} \Delta x}_{=1} \, f(\xi) = & \text{con $\xi \in [x, x+\Delta x]$, $f(x)$ continua} \\
        \vspace{-30pt} & & \text{$\left(\lim_{\Delta x \rightarrow 0} f(x+\Delta x) = \lim_{\Delta x \rightarrow 0} f(\xi) = f(x) \right)$} \\
        & = f(x)  \  .
    \end{aligned}
\end{equation}
\end{proof}

% ------------------------------------------------------------------------------
\section{Integrali fondamentali}
Partendo della tabella delle derivate fondamentali, si può usare il teorema fondamentale del calcolo infinitesimale (\ref{thm:fundamental-calculus}) per ``invertire l'operazione'' e ottenere una tabella di integrali fondamentali.
\begin{center}
    \begin{tabular}{lll}
        & \textbf{funzione} & \textbf{primitiva} \\
        Potenza      & $f(x) = x^a \, , \quad a \ne -1$ & $F(x) = \frac{1}{a+1} x^{a+1} + c$ \\
        Logaritmo    & $f(x) = \frac{1}{x}$             & $F(x) = \ln{|x|} + c$       \\
        Esponenziale & $f(x) = e^x$                     & $F(x) = e^x + c$          \\
        Seno         & $f(x) = \sin x$                  & $F(x) =-\cos x + c$       \\
        Coseno       & $f(x) = \cos x$                  & $F(x) = \sin x + c$       \\
    \end{tabular}
\end{center}

\section{Regole di integrazione}
\subsection{Integrazione per parti}
\begin{itemize}
 \item Definendo $F(x)$, $G(x)$ le primitive delle funzioni $f(x)$, e $g(x)$
 \item Integrando in $x$ dalla regola di \textbf{derivazione del prodotto} $(F(x)G(x))' = F'(x)G(x) + F(x)G'(x)$, riscritta isolando il termine $F'(x)G(x) = (F(x)G(x))' - F(x)G'(x)$
\end{itemize}
si ottiene
\begin{equation}
\begin{aligned}
    \int f(x) G(x) dx & = \int (F(x) G(x))' dx - \int F(x) G'(x) dx = \\
    &= F(x)G(x) - \int F(x) G'(x) dx 
\end{aligned}
\end{equation}

\begin{example}
\end{example}

\subsection{Integrazione con sostituzione}
\begin{itemize}
    \item Definendo la funzione composta $\overline{F}(x) = F(y(x))$ e le derivate
        \begin{equation}
            \overline{f}(x) = \dfrac{d}{dx} \overline{F}(x) \qquad , \qquad f(y) = \dfrac{d}{dy}F(y)
        \end{equation}
    \item Partendo dalla regola di \textbf{derivazione della funzione composta}, $\overline{F}(x) = F(y(x))$
        \begin{equation}
            \overline{f}(x) = \dfrac{d}{dx} \overline{F}(x) = \dfrac{d}{dx} F(y(x)) = \dfrac{d F}{dy}(y(x)) \dfrac{d y}{d x}(x) = f(y(x)) y'(x)
        \end{equation}
\end{itemize}
Usando il teorema fondamentale del calcolo infinitesimale
\begin{equation}
\begin{aligned}
    F(y) & = \int f(y) dy \\
    \overline{F}(x) & = \int \overline{f}(x) dx = \int f(y(x)) \ y'(x) dx
\end{aligned}
\end{equation}
Se si introduce la dipendenza $y(x)$ nella prima equazione, si ottiene l'uguaglianza tra le ultime due espressioni, $F(y(x)) = \overline{F}(x)$, e quindi
\begin{equation}
  \int f(y) dy = \int f(y(x)) \ y'(x) dx \ .
\end{equation}

\begin{example}
\end{example}
